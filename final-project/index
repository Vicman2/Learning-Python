import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd

#Get every data from the csv and remove any column with empty data
df = pd.read_csv('data_professional.csv').dropna()


# This is a function that takes in a number and converts it to float
#And If the value is not a number it returns the numpy nan value 
# Source: ChatGPT
def safe_convert_to_float(x):
    try:
        return float(x)
    except ValueError:
        return np.nan  # or handle the error as needed

#Convert the csv data to numpy
X = df.to_numpy()
print(len(X))

#Take the first 1315 colum but only select values on the 6, 9, 10, 11 and 12 index
#Where 
# 6 - Represents the Age
# 9 - Represents the leaves used. 
# 10 - Represents the leaves remaining 
# 11 - Represents the rating 
# 12 - Represents the past experience 
x_train = X[:1315, [6, 9, 10, 11, 12]].copy()
#Convert each individual entry to float
x_train = np.vectorize(safe_convert_to_float)(x_train)
# Pick the training 
y_train = X[:1315:, 7].copy()
#convert the salary to float, i.e convert the seventh column to a float
y_train = y_train.astype('float64')


#The same goes with the the test set but now, the other half of the data
x_test = X[1315:,[6, 9, 10, 11, 12]].copy()
x_test = np.vectorize(safe_convert_to_float)(x_test)
y_test = X[1315:,7].copy()
y_test = y_test.astype('float64')

# Scale target value to a reasonable range
y_test /= 10000
y_train /= 10000


num_features = x_train.shape[1]


# ############ The model ###############
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(num_features,)))

# Normalization layer to scale input features to mean 0, std 1.[[[[From Note]]]]
L = tf.keras.layers.Normalization(axis=1)
L.adapt(x_train) # Use the training set to determine scaling factors

model.add(L)
model.add(tf.keras.layers.Dense(128))
model.add(tf.keras.layers.LeakyReLU(negative_slope=0.1))
model.add(tf.keras.layers.Dense(64))
model.add(tf.keras.layers.LeakyReLU(negative_slope=0.1))
model.add(tf.keras.layers.Dropout(0.1))
model.add(tf.keras.layers.Dense(32))
model.add(tf.keras.layers.LeakyReLU(negative_slope=0.1))
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.LeakyReLU(negative_slope=0.1))
model.add(tf.keras.layers.Dense(1, activation="softplus"))
model.compile(optimizer="adam", loss="mse")

# [[[[From Note]]]] This idea of this callback is from our note from 12.3 
callback = tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=10,
restore_best_weights=True)
model.fit(x_train, y_train, epochs=100, validation_split=0.1,
callbacks=[callback], verbose=2)
history = model.history.history
# Evaluate on the test set.
h = model.evaluate(x_test, y_test, verbose=2)

# ###########################################################
# Plot the loss on train & validate sets by epoch:
num_epochs = len(history["loss"])
fig,ax = plt.subplots(1,2)
ep = range(1,num_epochs+1)
ax[0].plot(ep, history["loss"], color="black")
ax[0].plot(ep, history["val_loss"], color="blue")
ax[0].set(xlim=(1,num_epochs+1))
ax[0].set(title="Loss by epoch", xlabel="epoch", ylabel="MSE loss")
ax[0].legend(["train", "validation"], loc="upper left")

# Now plot the predictions vs true values on the test set:
y0,y1 = y_test.min(), y_test.max()
ax[1].plot([y0,y1],[y0,y1], color="red")
y_pred = model(x_test)
ax[1].scatter(y_test, y_pred,s=1)
ax[1].set(title=f"Test set (in $10k) (loss={h:0.2f})",
xlabel="actual", ylabel="predicted")
plt.show()


